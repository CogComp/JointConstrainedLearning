{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the actual MATRES data created/annotated by Qiang Ning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRES_timebank = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/timebank.txt'\n",
    "MATRES_aquaint = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/aquaint.txt'\n",
    "MATRES_platinum = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/platinum.txt'\n",
    "eiid_to_event_trigger_train = {}\n",
    "eiid_pair_to_label_train = {}\n",
    "eiid_to_event_trigger_test = {}\n",
    "eiid_pair_to_label_test = {}\n",
    "MATRES_data_cases_train = []\n",
    "MATRES_data_cases_test = []\n",
    "temp_label_map = {\"BEFORE\": 0, \"AFTER\": 1, \"EQUAL\": 2, \"VAGUE\": 3}\n",
    "\n",
    "def MATRES_READER(file, eiid_to_event_trigger, eiid_pair_to_label):\n",
    "    with open(file, \"r\") as f_matres:\n",
    "        content = f_matres.read().split(\"\\n\")\n",
    "        for rel in content:\n",
    "            rel = rel.split(\"\\t\")\n",
    "            fname = rel[0]\n",
    "            trigger1 = rel[1]\n",
    "            trigger2 = rel[2]\n",
    "            eiid1 = int(rel[3])\n",
    "            eiid2 = int(rel[4])\n",
    "            tempRel = temp_label_map[rel[5]]\n",
    "\n",
    "            if fname not in eiid_to_event_trigger:\n",
    "                eiid_to_event_trigger[fname] = {}\n",
    "                eiid_pair_to_label[fname] = {}\n",
    "            eiid_pair_to_label[fname][(eiid1, eiid2)] = tempRel\n",
    "            if eiid1 not in eiid_to_event_trigger.keys():\n",
    "                eiid_to_event_trigger[fname][eiid1] = trigger1\n",
    "            if eiid2 not in eiid_to_event_trigger.keys():\n",
    "                eiid_to_event_trigger[fname][eiid2] = trigger2\n",
    "\n",
    "MATRES_READER(MATRES_timebank, eiid_to_event_trigger_train, eiid_pair_to_label_train)  \n",
    "MATRES_READER(MATRES_aquaint, eiid_to_event_trigger_train, eiid_pair_to_label_train)  \n",
    "MATRES_READER(MATRES_platinum, eiid_to_event_trigger_test, eiid_pair_to_label_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if \"wsj_0695\" in eiid_to_event_trigger_train.keys():\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '2041' in eiid_to_event_trigger_train[\"APW19980322.0749\"]:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify whether token's first char matches the starting char position in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(token_list, My_Text, debugging):\n",
    "    count_error = 0\n",
    "    for token in token_list:\n",
    "        if debugging: print(token)\n",
    "        if token[1] == ' ':\n",
    "            continue\n",
    "        if token[1][0] != ' ':\n",
    "            if My_Text[token[2]] == token[1][0]: # token[2] is the starting char position in My_Text; token[1] is the word\n",
    "                count_error = count_error\n",
    "            else:\n",
    "                count_error += 1\n",
    "                print(token)\n",
    "        else:\n",
    "            if My_Text[token[2]] == token[1][1]:\n",
    "                count_error = count_error\n",
    "            else:\n",
    "                count_error += 1\n",
    "                print(token)\n",
    "    #print(\"count_error:\",count_error)\n",
    "    return count_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "bert_emb_path = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/bertEMB/'\n",
    "dict_path = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/event_mention/'\n",
    "token_dict_path = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/token_dict_path/'\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', unk_token='<unk>')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get RoBERTa embeddings for text in MATRES, the data is actually from TempEval3 https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html\n",
    "\n",
    "### We did not find the annotated file named \"nyt_20130321_sarcozy.tml\", but only find the original text. Hence we temporarily eliminate this file from MATRES \n",
    "/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/platinum.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_20130322_1150.tml\n",
      "verify success\n",
      "CNN_20130322_314.tml\n",
      "verify success\n",
      "bbc_20130322_721.tml\n",
      "verify success\n",
      "nyt_20130322_strange_computer.tml\n",
      "verify success\n",
      "bbc_20130322_332.tml\n",
      "verify success\n",
      "nyt_20130321_china_pollution.tml\n",
      "verify success\n",
      "nyt_20130321_cyprus.tml\n",
      "verify success\n",
      "WSJ_20130321_1145.tml\n",
      "verify success\n",
      "CNN_20130321_821.tml\n",
      "verify success\n",
      "WSJ_20130318_731.tml\n",
      "verify success\n",
      "nyt_20130321_sarkozy.tml\n",
      "verify success\n",
      "CNN_20130322_1003.tml\n",
      "verify success\n",
      "bbc_20130322_1600.tml\n",
      "verify success\n",
      "AP_20130322.tml\n",
      "verify success\n",
      "bbc_20130322_1353.tml\n",
      "verify success\n",
      "CNN_20130322_1243.tml\n",
      "verify success\n",
      "CNN_20130322_248.tml\n",
      "verify success\n",
      "WSJ_20130322_804.tml\n",
      "verify success\n",
      "nyt_20130321_women_senate.tml\n",
      "verify success\n",
      "WSJ_20130322_159.tml\n",
      "verify success\n",
      "longest_sent: 81\n"
     ]
    }
   ],
   "source": [
    "mypath_TB = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/TBAQ-cleaned/TimeBank/' # after correction\n",
    "onlyfiles_TB = [f for f in listdir(mypath_TB) if isfile(join(mypath_TB, f))]\n",
    "mypath_AQ = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/TBAQ-cleaned/AQUAINT/' \n",
    "onlyfiles_AQ = [f for f in listdir(mypath_AQ) if isfile(join(mypath_AQ, f))]\n",
    "mypath_PL = '/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/te3-platinum/'\n",
    "onlyfiles_PL = [f for f in listdir(mypath_PL) if isfile(join(mypath_PL, f))]\n",
    "\n",
    "onlyfiles = onlyfiles_PL\n",
    "#onlyfiles = [\"WSJ_20130322_159.tml\"]\n",
    "mypath = mypath_PL\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\"\"\"\n",
    "file = \"./pos-tags.txt\"\n",
    "pos_tags_list = []\n",
    "with open(file, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for line in content:\n",
    "        pos_tags_list.append(line.split(\" \")[4])\n",
    "        \n",
    "tag_to_num = {tag:i for i, tag in enumerate(sorted(pos_tags_list))} \n",
    "num_to_tag = {i:tag for i, tag in enumerate(sorted(pos_tags_list))}        \n",
    "        \n",
    "# define example\n",
    "values = array(pos_tags_list)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n",
    "\n",
    "zero_35 = [0.] * 35\n",
    "zero_35 = numpy.asarray(zero_35)\n",
    "def POSTAG_TO_ONEHOT(pos_tag, onehot_encoded, tag_to_num):\n",
    "    if pos_tag in tag_to_num.keys():\n",
    "        num = tag_to_num[pos_tag]\n",
    "        return onehot_encoded[num]\n",
    "    else:\n",
    "        return zero_35\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pickle\n",
    "POSTAG_ONEHOT_dict = pickle.load(open(\"/shared/why16gzl/logic_driven/EMNLP-2020/MATRES/TBAQ-cleaned/POSTAG_ONEHOT.dict\", \"rb\"))\n",
    "zero_18 = [0.] * 18\n",
    "zero_18 = numpy.asarray(zero_18)\n",
    "def POSTAG_TO_ONEHOT(pos_tag):\n",
    "    if pos_tag in POSTAG_ONEHOT_dict.keys():\n",
    "        return POSTAG_ONEHOT_dict[pos_tag]\n",
    "    else:\n",
    "        return zero_18\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "count = 0\n",
    "debugging = 0\n",
    "longest_sent = 0\n",
    "for fname in onlyfiles:\n",
    "    print(fname)\n",
    "    count += 1\n",
    "    #print(count)\n",
    "    tree = ET.parse(mypath+fname)\n",
    "    root = tree.getroot()\n",
    "    MY_STRING = str(ET.tostring(root))\n",
    "    \n",
    "    # eID is the id for event in each article; eiid is the event id in the timebank/aquaint data\n",
    "    # <MAKEINSTANCE eventID=\"e1\" eiid=\"ei415\" tense=\"PRESENT\" aspect=\"PERFECTIVE\" polarity=\"POS\" pos=\"VERB\" />\n",
    "    eID_to_eiid = {}\n",
    "    article_event = []\n",
    "    for makeinstance in root.findall('MAKEINSTANCE'):\n",
    "        instance_str = str(ET.tostring(makeinstance)).split(\" \")\n",
    "        try:\n",
    "            assert instance_str[3].split(\"=\")[0] == \"eventID\"\n",
    "            assert instance_str[2].split(\"=\")[0] == \"eiid\"\n",
    "            eiid = int(instance_str[2].split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "            eID = instance_str[3].split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "        except:\n",
    "            for i in instance_str:\n",
    "                if i.split(\"=\")[0] == \"eventID\":\n",
    "                    eID = i.split(\"=\")[1].replace(\"\\\"\", \"\")\n",
    "                if i.split(\"=\")[0] == \"eiid\":\n",
    "                    eiid = int(i.split(\"=\")[1].replace(\"\\\"\", \"\")[2:])\n",
    "        eID_to_eiid[eID] = eiid\n",
    "    #print(\"len(eID_to_eiid)\",len(eID_to_eiid))    \n",
    "    start = MY_STRING.find(\"<TEXT>\") + 6\n",
    "    end = MY_STRING.find(\"</TEXT>\")\n",
    "    MY_TEXT = MY_STRING[start:end]\n",
    "    while MY_TEXT[0] == \" \":\n",
    "        MY_TEXT = MY_TEXT[1:]\n",
    "    MY_TEXT = MY_TEXT.replace(\"\\\\n\", \" \")\n",
    "    MY_TEXT = MY_TEXT.replace(\"\\\\'\", \"\\'\")\n",
    "    MY_TEXT = MY_TEXT.replace(\"  \", \" \")\n",
    "    MY_TEXT = MY_TEXT.replace(\" ...\", \"...\")\n",
    "    \n",
    "    # eID is the id for event in each article; charID is the starting char position for each event\n",
    "    # <EVENT eid=\"e1\" class=\"I_ACTION\">predicted</EVENT>\n",
    "    eID_to_charID = {}\n",
    "    while MY_TEXT.find(\"<\") != -1:\n",
    "        start = MY_TEXT.find(\"<\")\n",
    "        end = MY_TEXT.find(\">\")\n",
    "        if MY_TEXT[start + 1] == \"E\":\n",
    "            event_description = MY_TEXT[start:end].split(\" \")\n",
    "            eid = (event_description[2].split(\"=\"))[1].replace(\"\\\"\", \"\")\n",
    "            eID_to_charID[eid] = start\n",
    "            MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n",
    "        else:\n",
    "            MY_TEXT = MY_TEXT[:start] + MY_TEXT[(end + 1):]\n",
    "    #print(\"eID_to_charID: \", eID_to_charID)    \n",
    "    if debugging: print(MY_TEXT)\n",
    "    # k = 695 # the 695-th character is: s\n",
    "    # said: eiid = 431, eID = e26 appears in ABC19980120.1830.0957\n",
    "    #print(MY_TEXT[k-5:k+5]) # \" has said \"\n",
    "    #print(eiid_to_event_trigger['430']) eiid 430 appears in APW19980227.0487\n",
    "    # so we need to construct a dictionary for each article\n",
    "    \n",
    "    # verify whether token's first char matches the starting char position in the article\n",
    "    # eID_to_charID is within the context of each article \n",
    "    if len(onlyfiles) > 30:\n",
    "        eiid_to_event_trigger = eiid_to_event_trigger_train\n",
    "    else:\n",
    "        eiid_to_event_trigger = eiid_to_event_trigger_test\n",
    "    count_error = 0\n",
    "    #print(eID_to_eiid)\n",
    "    ############################################################################\n",
    "    #eiid_to_event_trigger = eiid_to_event_trigger_train\n",
    "    for eID, charID in eID_to_charID.items():\n",
    "        file_name = fname.replace(\".tml\", \"\")\n",
    "        #print(eiid_to_event_trigger[file_name])\n",
    "        if eID in eID_to_eiid.keys():\n",
    "            #print(eID)\n",
    "            if file_name in eiid_to_event_trigger.keys():\n",
    "                #print(file_name)\n",
    "                if eID_to_eiid[eID] in eiid_to_event_trigger[file_name].keys():\n",
    "                    #print(\"eID_to_eiid: \", eID_to_eiid[eID])\n",
    "                    trigger = eiid_to_event_trigger[file_name][eID_to_eiid[eID]]\n",
    "                    # eID, eiid, charID, trigger word\n",
    "                    article_event.append((eID, eID_to_eiid[eID], charID, trigger))\n",
    "                    if MY_TEXT[charID] == trigger[0]:\n",
    "                        count_error = count_error\n",
    "                    else:\n",
    "                        count_error += 1\n",
    "    if count_error == 0:\n",
    "        count_error = 0\n",
    "    else:\n",
    "        print(\"failed!!!!!!!!!!!!!!\")\n",
    "    if debugging: print(article_event)\n",
    "    sent_tokenized_text = sent_tokenize(MY_TEXT)\n",
    "    token_index = 0\n",
    "    sent_index = 0\n",
    "    token_list = []\n",
    "    token_dict = {}\n",
    "    sent_emb_list = []\n",
    "    span_counter = 0\n",
    "    for sent in sent_tokenized_text:\n",
    "        #if debugging: print(sent)\n",
    "        # '\\n' and ' ' are both one char, so it doesn't influence the span starting position \n",
    "        #sent_to_be_encoded = sent.replace('\\n', '')\n",
    "        if debugging: print(sent)\n",
    "            \n",
    "        ### POS Tag One-Hot \n",
    "        tok=nltk.tokenize.word_tokenize(sent) \n",
    "        pos=nltk.pos_tag(tok)\n",
    "        sent_pos_tag_dict = {}\n",
    "        sent_word_dict = {}\n",
    "        index_here = 0\n",
    "        for (word, POS) in pos:\n",
    "            ord_first_char = ord(word[0])\n",
    "            POS_ONEHOT = POSTAG_TO_ONEHOT(POS)\n",
    "            if POS_ONEHOT.sum() > 0 and ((ord_first_char >= 65 and ord_first_char <= 90) or (ord_first_char >= 97 and ord_first_char <= 122) or (ord_first_char >= 48 and ord_first_char <= 57)):\n",
    "                sent_pos_tag_dict[index_here] = POSTAG_TO_ONEHOT(POS)\n",
    "                sent_word_dict[index_here] = word\n",
    "                index_here += 1\n",
    "\n",
    "        encoded = tokenizer.encode(sent) \n",
    "        if len(encoded) > longest_sent:\n",
    "            longest_sent = len(encoded)\n",
    "        input_ids = torch.tensor(encoded).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "        last_hidden_states = last_hidden_states.view(-1, 768)\n",
    "        #sent_emb_list.append(last_hidden_states.view(-1, 768))\n",
    "        #print(last_hidden_states.view(-1, 768).size())\n",
    "        token_num = list(last_hidden_states.size())[0]\n",
    "        POS_TAG_ONEHOT = torch.zeros(token_num, 18)\n",
    "\n",
    "        word_index = 0\n",
    "        current_word_index = 0\n",
    "\n",
    "        for index, i in enumerate(encoded):\n",
    "            list_ = [i]\n",
    "            token = tokenizer.decode(list_)\n",
    "            if token == \"<s>\" or token == \"</s>\":\n",
    "                #if debugging: print(token)\n",
    "                continue\n",
    "            if token[0] == \" \": \n",
    "                token_word = token[1:]\n",
    "                if word_index in sent_word_dict:\n",
    "                    #print(sent_word_dict[word_index])\n",
    "                    #print(sent_word_dict)\n",
    "                    if sent_word_dict[word_index][0] == token[1]:\n",
    "                        POS_TAG_ONEHOT[index] = torch.tensor(sent_pos_tag_dict[word_index])\n",
    "                        current_word_index = word_index\n",
    "                        word_index += 1\n",
    "                span_start = span_counter + 1\n",
    "            else:\n",
    "                POS_TAG_ONEHOT[index] = torch.tensor(sent_pos_tag_dict[current_word_index])\n",
    "                span_start = span_counter\n",
    "                token_word = token\n",
    "            span_counter += len(token)\n",
    "            span_end = span_counter - 1\n",
    "            if debugging: print(token_index, token, span_start, span_end)\n",
    "            token_list.append((token_index, token, span_start, span_end, 0))\n",
    "            token_dict[span_start] = [index, token, span_end, 0, sent_index]\n",
    "            if debugging: print(\"token:\",token)\n",
    "            if debugging: print(\"span_start:\",span_start)\n",
    "            assert token_word[0] == MY_TEXT[span_start]\n",
    "            token_index += 1\n",
    "\n",
    "        sent_emb_list.append(torch.cat((last_hidden_states, POS_TAG_ONEHOT), dim=1))     \n",
    "        sent_index += 1\n",
    "        \n",
    "    #print(\"number of tokens:\", len(token_list))\n",
    "    event_mention_dict = {}\n",
    "    #print(\"len(article_event):\",len(article_event))\n",
    "    for art_eve in article_event: # article_event.append((eID, eID_to_eiid[eID], charID, trigger))\n",
    "        if debugging: \n",
    "            print(int(art_eve[2]))\n",
    "        token_dict[int(art_eve[2])][3] = 1\n",
    "        eiid = int(art_eve[1])\n",
    "        eID = art_eve[0]\n",
    "        #print(\"eiid:\",eiid)\n",
    "        #<EventMentionInfo> <ID>25</ID> <AnchorText>took</AnchorText> <Type>Occurrence</Type> <Position>1154</Position </EventMentionInfo>\n",
    "        # event_id, BertTokenized_token_id, sent_id, event_mention\n",
    "        # key: eID; [token_index, sent_index, token, eiid, trigger]\n",
    "        event_mention_dict[eiid] = [token_dict[int(art_eve[2])][0], token_dict[int(art_eve[2])][4], token_dict[int(art_eve[2])][1], eID_to_eiid[eID], eiid_to_event_trigger[fname.replace(\".tml\", \"\")][eID_to_eiid[eID]]]\n",
    "    #print(token_dict)\n",
    "    if not verify(token_list, MY_TEXT, debugging):\n",
    "        print(\"verify success\")\n",
    "    else:\n",
    "        print(\"verify fail\")\n",
    "             \n",
    "    #print(event_mention_dict)\n",
    "    with open(bert_emb_path + fname.replace('.tml', '.pickle'), 'wb') as file_out:\n",
    "        c = torch.ones(120, 768+18)\n",
    "        sent_emb_list.append(c)\n",
    "        art_emb = pad_sequence(sent_emb_list, batch_first=True)\n",
    "        pickle.dump(art_emb, file_out)\n",
    "        \n",
    "    with open(dict_path + fname.replace('.tml', '.event'), 'wb') as f_out:\n",
    "        pickle.dump(event_mention_dict, f_out)\n",
    "    \n",
    "    with open(token_dict_path + fname.replace('.tml', '.tokenDICT'), 'wb') as f_out:\n",
    "        pickle.dump(token_dict, f_out)\n",
    "        \n",
    "print(\"longest_sent:\", longest_sent)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function of the cell below is the same as the first half of:\n",
    "/home1/w/why16gzl/KAIROS/event_abstraction/baselines/train_test_gen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_file = 0\n",
    "for fname in eiid_to_event_trigger_train.keys():\n",
    "    print(fname)\n",
    "    count_file += 1\n",
    "    with open(dict_path+fname+\".event\", \"rb\") as f:\n",
    "        event_mention_dict = pickle.load(f)\n",
    "        print(event_mention_dict)\n",
    "    eiid_to_event_trigger_dict = eiid_to_event_trigger_train[fname]\n",
    "    #x: eiid; \n",
    "    for x in eiid_to_event_trigger_dict.keys():\n",
    "        for y in eiid_to_event_trigger_dict.keys():\n",
    "            for z in eiid_to_event_trigger_dict.keys():\n",
    "                if x!=y and y!=z and x!=z:\n",
    "                    if (x, y) in eiid_pair_to_label_train[fname].keys() and (y, z) in eiid_pair_to_label_train[fname].keys() and (x, z) in eiid_pair_to_label_train[fname].keys():\n",
    "                        assert type(x) == type(1)\n",
    "                        xy = eiid_pair_to_label_train[fname][(x, y)]\n",
    "                        yz = eiid_pair_to_label_train[fname][(y, z)]\n",
    "                        xz = eiid_pair_to_label_train[fname][(x, z)]\n",
    "                        MATRES_data_cases_train.append((count_file, x, y, z, event_mention_dict[x][0], event_mention_dict[y][0], event_mention_dict[z][0], xy, yz, xz, -1, -1, fname))\n",
    "                        \n",
    "for fname in eiid_to_event_trigger_test.keys():\n",
    "    print(fname)\n",
    "    count_file += 1\n",
    "    with open(dict_path+fname+\".event\", \"rb\") as f:\n",
    "        event_mention_dict = pickle.load(f)\n",
    "    eiid_to_event_trigger_dict = eiid_to_event_trigger_test[fname]\n",
    "    #print(\"eiid_to_event_trigger_dict\", eiid_to_event_trigger_dict)\n",
    "    #print(\"event_mention_dict:\", event_mention_dict)\n",
    "    for x in eiid_to_event_trigger_dict.keys():\n",
    "        for y in eiid_to_event_trigger_dict.keys():\n",
    "            if x!=y:\n",
    "                if (x, y) in eiid_pair_to_label_test[fname].keys():\n",
    "                    xy = eiid_pair_to_label_test[fname][(x, y)]\n",
    "                    if fname == \"WSJ_20130322_159\":\n",
    "                        assert type(x) == type(1)\n",
    "                        #print(eiid_to_event_trigger_test[fname])\n",
    "                    MATRES_data_cases_test.append((count_file, x, y, x, event_mention_dict[x][0], event_mention_dict[y][0], -1, xy, xy, xy, -1, -1, fname))\n",
    "                    \n",
    "print(len(MATRES_data_cases_train), 'event triples out of', len(eiid_to_event_trigger_train.keys()), 'articles.')\n",
    "print(len(MATRES_data_cases_test), 'event pairs out of', len(eiid_to_event_trigger_test.keys()), 'articles.')\n",
    "with open(\"/shared/why16gzl/logic_driven/EMNLP-2020/MATRES_data_cases_train.pickle\", 'wb') as fOUT:\n",
    "    pickle.dump(MATRES_data_cases_train, fOUT)\n",
    "with open(\"/shared/why16gzl/logic_driven/EMNLP-2020/MATRES_data_cases_test.pickle\", 'wb') as fOUT:\n",
    "    pickle.dump(MATRES_data_cases_test, fOUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consistency_env",
   "language": "python",
   "name": "consistency_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
